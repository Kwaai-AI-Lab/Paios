{
    "id": "airllm",
    "title": "AirLLM",
    "description": "AirLLM optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card. No quantization, distillation, pruning or other model compression techniques that would result in degraded model performance are needed.",
    "author": {
        "name": "Gavin Li",
        "url": "https://github.com/lyogavin/Anima/tree/main/air_llm"
    },
    "maintainer": {
        "name": "Sam Johnston",
        "email": "samj@samj.net",
        "url": "http://samjohnston.org"
    },
    "language": "python",
    "scripts": {
        "start": "main.py"
    },
    "license": "Apache-2.0",
    "dependencies": {
        "python": [
            {
                "id": "airllm",
                "name": "AirLLM",
                "version": ">=2.8"
            }
        ]
    }
}
